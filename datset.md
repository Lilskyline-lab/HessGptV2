# Configuration des Datasets pour Fine-tuning LoRA

## ğŸ“‹ Vue d'ensemble du projet

**Objectif :** Fine-tuner un modÃ¨le de langage en LoRA pour :
- Conversations naturelles (franÃ§ais/anglais)
- Function calling (requÃªtes HTTP)
- Recherche d'informations externes

---

## ğŸ¯ Datasets pour le Fine-tuning

### 1. Conversations gÃ©nÃ©rales - QualitÃ© Premium

#### **Anthropic/hh-rlhf**
- **Langue :** Anglais
- **Taille :** ~160k conversations
- **Usage :** Conversations helpful & harmless de haute qualitÃ©
- **Pourquoi :** Ton naturel, empathique et sÃ©curisÃ©

#### **HuggingFaceH4/ultrachat_200k**
- **Langue :** Anglais
- **Taille :** 200k conversations
- **Usage :** DiversitÃ© de sujets conversationnels
- **Pourquoi :** Excellente qualitÃ© synthÃ©tique, large couverture

#### **OpenAssistant/oasst2**
- **Langue :** Multilingue (anglais + franÃ§ais)
- **Taille :** ~160k messages
- **Usage :** Conversations multi-tours validÃ©es par la communautÃ©
- **Pourquoi :** DonnÃ©es rÃ©elles, votes de qualitÃ©

### 2. Instructions en franÃ§ais

#### **bofenghuang/vigogne-instruction-following-v1.0**
- **Langue :** FranÃ§ais
- **Taille :** ~50k instructions
- **Usage :** Renforcer les capacitÃ©s en franÃ§ais
- **Pourquoi :** Meilleur dataset d'instructions franÃ§aises

### 3. Function Calling & Tool Use

#### **xlangai/xlam-function-calling-60k**
- **Langue :** Anglais
- **Taille :** 60k exemples
- **Usage :** Apprendre Ã  identifier et structurer les appels de fonctions
- **Pourquoi :** Dataset le plus complet pour function calling

#### **glaiveai/glaive-function-calling-v2**
- **Langue :** Anglais
- **Taille :** ~110k exemples
- **Usage :** ComplÃ©ter l'apprentissage des function calls
- **Pourquoi :** DiversitÃ© de fonctions et scÃ©narios rÃ©els

#### **Nexusflow/NexusRaven-V2-data**
- **Langue :** Anglais
- **Taille :** Variable
- **Usage :** SpÃ©cialisation tool use avancÃ©
- **Pourquoi :** Focus sur l'exÃ©cution prÃ©cise d'outils

---

## ğŸ”¤ Dataset pour l'entraÃ®nement du Tokenizer

### **allenai/c4**
- **Langue :** Multilingue (c4-en + c4-fr)
- **Taille :** ~300+ Go par langue
- **Usage :** EntraÃ®ner un tokenizer BPE Ã  50k vocab
- **Pourquoi :** 
  - Corpus web massif et diversifiÃ©
  - NettoyÃ© et filtrÃ© pour la qualitÃ©
  - Vocabulaire reprÃ©sentatif du web rÃ©el
  - Inclut franÃ§ais et anglais

**Portions recommandÃ©es :**
- 60% C4 anglais (c4-en)
- 40% C4 franÃ§ais (c4-fr)

---

## ğŸ“Š RÃ©partition recommandÃ©e du fine-tuning

```
Conversations gÃ©nÃ©rales (70%)
â”œâ”€â”€ ultrachat_200k          â†’ 30%
â”œâ”€â”€ Anthropic/hh-rlhf       â†’ 25%
â”œâ”€â”€ oasst2                  â†’ 10%
â””â”€â”€ vigogne (franÃ§ais)      â†’ 5%

Function Calling (30%)
â”œâ”€â”€ xlam-function-calling   â†’ 15%
â”œâ”€â”€ glaive-function-calling â†’ 10%
â””â”€â”€ NexusRaven-V2           â†’ 5%
```

---

## ğŸ› ï¸ Pipeline d'entraÃ®nement

### Ã‰tape 1 : Tokenizer
1. TÃ©lÃ©charger Ã©chantillons de `allenai/c4` (en + fr)
2. EntraÃ®ner tokenizer BPE 50k vocab
3. Tokens spÃ©ciaux : `<pad>`, `<s>`, `</s>`, `<unk>`, `<search>`, `</search>`

### Ã‰tape 2 : Fine-tuning LoRA
1. Charger tous les datasets listÃ©s ci-dessus
2. Normaliser au format conversations uniforme
3. MÃ©langer selon les proportions recommandÃ©es
4. Fine-tuner avec LoRA (rank 16-64)

---

## âš™ï¸ ParamÃ¨tres suggÃ©rÃ©s

**Tokenizer :**
- Vocab size : 50,000
- Algorithm : BPE (Byte-Pair Encoding)
- Training data : 10-50 Go de C4 (mix en/fr)

**LoRA Fine-tuning :**
- Rank (r) : 32-64
- Alpha : 64-128
- Dropout : 0.05-0.1
- Target modules : q_proj, v_proj, k_proj, o_proj
- Epochs : 1-3 selon la taille totale

---

## ğŸ“ Notes importantes

- **Filtrage :** Filtrer oasst2 pour garder uniquement franÃ§ais/anglais
- **QualitÃ© :** VÃ©rifier et nettoyer les datasets avant entraÃ®nement
- **Ã‰quilibrage :** Ajuster les proportions selon vos besoins spÃ©cifiques
- **Validation :** Garder 5-10% de chaque dataset pour validation

---

## ğŸ”— Liens Hugging Face

- Anthropic/hh-rlhf : `https://huggingface.co/datasets/Anthropic/hh-rlhf`
- ultrachat_200k : `https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k`
- oasst2 : `https://huggingface.co/datasets/OpenAssistant/oasst2`
- vigogne : `https://huggingface.co/datasets/bofenghuang/vigogne-instruction-following-v1.0`
- xlam-function-calling : `https://huggingface.co/datasets/xlangai/xlam-function-calling-60k`
- glaive-function-calling : `https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2`
- NexusRaven : `https://huggingface.co/datasets/Nexusflow/NexusRaven-V2-data`
- C4 : `https://huggingface.co/datasets/allenai/c4`

---

**Date de crÃ©ation :** 27 octobre 2025  
**Version :** 1.0